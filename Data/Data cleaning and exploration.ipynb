{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48c64b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle \n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e030d906",
   "metadata": {},
   "source": [
    "## NOAA dataset importing and cleaning \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d6a93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data, removing unecessary columns \n",
    "raw_data = pd.read_csv(\"./Data import/dataset.csv\")\n",
    "raw_data = raw_data.drop(['Unnamed: 0', \"year\", \"month\", \"day\",'original_central_meridian_dist', 'original_latitude','integrated_flux', \"event_starttime\", \"event_peaktime\", \"event_endtime\", 'noaa_ar'], axis = 1)\n",
    "raw_data.matchtime = pd.to_datetime(raw_data.matchtime, format = \"%Y%m%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36161fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60ce1e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished querying row 0\n",
      "finished querying row 1000\n",
      "finished querying row 2000\n",
      "finished querying row 3000\n",
      "finished querying row 4000\n",
      "finished querying row 5000\n",
      "finished querying row 6000\n",
      "finished querying row 7000\n",
      "finished querying row 8000\n",
      "finished querying row 9000\n",
      "finished querying row 10000\n",
      "finished querying row 11000\n",
      "finished querying row 12000\n",
      "finished querying row 13000\n",
      "finished querying row 14000\n",
      "finished querying row 15000\n",
      "finished querying row 16000\n",
      "finished querying row 17000\n",
      "finished querying row 18000\n",
      "finished querying row 19000\n",
      "finished querying row 20000\n",
      "finished querying row 21000\n",
      "finished querying row 22000\n"
     ]
    }
   ],
   "source": [
    "## Add Mcintosh evolution \n",
    "\n",
    "# Write query function\n",
    "def query_mcintosh(noaa_ar, date, nhours): \n",
    "    try:\n",
    "        query_date = str(date - pd.DateOffset(hours= nhours))[0:10]\n",
    "        key = raw_data.query(f\"matchtime == '{query_date}' & noaa_ar_no == {noaa_ar}\")[\"mcintosh\"].iloc[0]\n",
    "    except:\n",
    "        key = \"None\"\n",
    "    return key\n",
    "\n",
    "## Query all the datapoints to find all previous Mcintosh classifications \n",
    "mcintosh_evolution = []\n",
    "for i in range(raw_data.shape[0]): \n",
    "    if i%1000 == 0: \n",
    "        print(f\"finished querying row {i}\")\n",
    "    query_row = raw_data.iloc[i]\n",
    "    if query_row[[\"goes_class\"]].isna()[0] == True: \n",
    "        mcintosh_evolution.append(\"None\")\n",
    "    else: \n",
    "        noaa_ar = query_row.noaa_ar_no\n",
    "        date = query_row.matchtime\n",
    "        previous_mcintosh = query_mcintosh(noaa_ar, date, 24)\n",
    "        mcintosh_evolution.append(previous_mcintosh)\n",
    "\n",
    "mcintosh_evolution = pd.Series(mcintosh_evolution)        \n",
    "# Create new column \n",
    "raw_data[\"mcintosh_evolution\"] = mcintosh_evolution + \"-\" + raw_data[\"mcintosh\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe60b18a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['B', 'C', 'M', 'X'], dtype=object), array([3806, 6068,  634,   42], dtype=int64))\n",
      "(10550, 14)\n",
      "index                      0\n",
      "noaa_ar_no                 0\n",
      "central_meridian_dist      0\n",
      "latitude                   0\n",
      "carrington_longitude       2\n",
      "corr_whole_spot_area     266\n",
      "mcintosh                   0\n",
      "LL                       266\n",
      "number_of_spots          266\n",
      "greenwich                  0\n",
      "matchtime                  0\n",
      "goes_class_ind             0\n",
      "goes_class                 0\n",
      "mcintosh_evolution         0\n",
      "dtype: int64\n",
      "\n",
      "not a substantial difference in dataset after removing missing data:\n",
      "(array(['B', 'C', 'M', 'X'], dtype=object), array([3663, 5952,  627,   42], dtype=int64))\n",
      "(10284, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "level_0                  0\n",
       "index                    0\n",
       "noaa_ar_no               0\n",
       "central_meridian_dist    0\n",
       "latitude                 0\n",
       "carrington_longitude     0\n",
       "corr_whole_spot_area     0\n",
       "mcintosh                 0\n",
       "LL                       0\n",
       "number_of_spots          0\n",
       "greenwich                0\n",
       "matchtime                0\n",
       "goes_class_ind           0\n",
       "goes_class               0\n",
       "mcintosh_evolution       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rid of missing data\n",
    "clean_data = raw_data.dropna(axis = 0, subset=['goes_class']).reset_index()\n",
    "clean_data = clean_data.query(\"goes_class_ind != 'A'\")\n",
    "print(np.unique(clean_data.goes_class_ind, return_counts = True))\n",
    "print(clean_data.shape)\n",
    "print(np.sum(clean_data.isna()))\n",
    "clean_data = clean_data.dropna(axis = 0, subset=['corr_whole_spot_area']).reset_index()\n",
    "print()\n",
    "print(\"not a substantial difference in dataset after removing missing data:\")\n",
    "print(np.unique(clean_data.goes_class_ind, return_counts = True))\n",
    "print(clean_data.shape)\n",
    "np.sum(clean_data.isna())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504f6934",
   "metadata": {},
   "source": [
    "## NOAA dataset data exploration \n",
    "visualisation of the distributions of dependant and independant features in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bfc4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphing Mcintosh classifications \n",
    "clean_data['mcintosh'].value_counts().head(30).plot(kind='barh', figsize=(20,10), title = \"Counts of the 30 most frequent Mcintosh classifications within the dataset\")\n",
    "# Graphing Mcintosh evolutions\n",
    "clean_data['mcintosh_evolution'].value_counts().head(30).plot(kind='barh', color = \"lightblue\", figsize=(20,10), title = \"Counts of the 30 most frequent Mcintosh evolutions within the dataset\")\n",
    "# Other independant features \n",
    "for column in clean_data.columns: \n",
    "    sns.displot(data=clean_data, x= column, color = \"red\")\n",
    "    \n",
    "# Solar flare frequency \n",
    "clean_data['goes_class_ind'].value_counts().plot(kind='bar', figsize=(10,10), title = \"Solar flare frequency within dataset\", color = \"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fb3e07",
   "metadata": {},
   "source": [
    "## NOAA dataset exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e5d498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Mcintosh evolution dataset \n",
    "data_evolution = clean_data.drop([\"mcintosh\"], axis = 1)\n",
    "data_evolution = pd.concat([data_evolution, pd.get_dummies(data_evolution[\"greenwich\"]), pd.get_dummies(data_evolution[\"mcintosh_evolution\"])], axis = 1)\n",
    "data_evolution = data_evolution.drop([\"mcintosh_evolution\", \"greenwich\"], axis = 1)\n",
    "# Get rid of hot encodes that are less than 1 in value (basically non existent)\n",
    "data_evolution.drop([col for col, val in data_evolution.sum().iteritems() if type(val) != str and val < 3], axis=1, inplace=True)\n",
    "\n",
    "## Create normal dataset\n",
    "data_1 = clean_data.drop([\"mcintosh_evolution\"], axis = 1)\n",
    "data_1 = pd.concat([data_1, pd.get_dummies(data_1[\"greenwich\"]), pd.get_dummies(data_1[\"mcintosh\"])], axis = 1)\n",
    "data_1 = data_1.drop([\"mcintosh\", \"greenwich\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1d080a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "## Create C excluded and B excluded flare datasets \n",
    "data_c = data_1[(data_1.goes_class_ind != \"B\") ].copy()\n",
    "data_b = data_1[data_1.goes_class_ind != \"C\"].copy()\n",
    "\n",
    "# Clean one more time before exporting \n",
    "for data in [data_1, data_c, data_b, data_evolution]: \n",
    "    data.goes_class_ind[data.goes_class_ind == \"B\"] = \"B/C\"\n",
    "    data.goes_class_ind[data.goes_class_ind == \"C\"] = \"B/C\"\n",
    "    data.set_index(data['matchtime'], inplace = True)\n",
    "    data.drop(['level_0', 'index', 'noaa_ar_no', 'matchtime','goes_class'], axis = 1, inplace = True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acce6252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['central_meridian_dist', 'latitude', 'carrington_longitude',\n",
       "       'corr_whole_spot_area', 'LL', 'number_of_spots', 'goes_class_ind',\n",
       "       'Alpha', 'Beta', 'Beta-Delta', 'Beta-Gamma', 'Beta-Gamma-Delta', 'Axx',\n",
       "       'Bxi', 'Bxo', 'Cai', 'Cao', 'Chi', 'Cho', 'Cki', 'Cko', 'Cri', 'Cro',\n",
       "       'Csi', 'Cso', 'Dac', 'Dai', 'Dao', 'Dhc', 'Dhi', 'Dho', 'Dkc', 'Dki',\n",
       "       'Dko', 'Dri', 'Dro', 'Dsc', 'Dsi', 'Dso', 'Eac', 'Eai', 'Eao', 'Ehc',\n",
       "       'Ehi', 'Eho', 'Ekc', 'Eki', 'Eko', 'Eri', 'Ero', 'Esc', 'Esi', 'Eso',\n",
       "       'Fac', 'Fai', 'Fao', 'Fhc', 'Fhi', 'Fho', 'Fkc', 'Fki', 'Fko', 'Fsc',\n",
       "       'Fsi', 'Fso', 'Hax', 'Hhx', 'Hkx', 'Hrx', 'Hsx'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8131128e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Export data \n",
    "for data, name in zip([data_1, data_c, data_b, data_evolution], [\"data_all\",\"data_only_c\", \"data_only_b\", \"data_evolution\"]):\n",
    "    # Split training and testing into before 2015 and after 2015\n",
    "    training = data[:\"2015-01-01\"]\n",
    "    testing = data[\"2015-01-01\":]\n",
    "    X_train = np.array(training.drop([\"goes_class_ind\"], axis = 1))\n",
    "    X_test = np.array(testing.drop([\"goes_class_ind\"], axis = 1))\n",
    "    # Make sure all numerical values are from -1 to 1 \n",
    "    max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "    X_train = max_abs_scaler.fit_transform(X_train)\n",
    "    X_test = max_abs_scaler.fit_transform(X_test)\n",
    "    # Get feature index and y \n",
    "    feature_index = data.drop([\"goes_class_ind\"], axis = 1).columns \n",
    "    y_train = np.array(training.goes_class_ind)\n",
    "    y_test = np.array(testing.goes_class_ind)\n",
    "\n",
    "\n",
    "    # Exporting data out \n",
    "    export = X_train, X_test, y_train, y_test, feature_index \n",
    "    pickle_out = open(\"clean_data/\" + name + \".pickle\", \"wb\")\n",
    "    pickle.dump(export, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851f744d",
   "metadata": {},
   "source": [
    "## SHARP data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de15a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data = pd.read_csv(\"./data_import/SHARP_data.csv\")\n",
    "# for 48 hours:\n",
    "raw_data = pd.read_csv(\"./data_import/SHARP_data_48.csv\")\n",
    "raw_data = raw_data.drop(['goes_sat', 'goes_channel',\"matchtime\" ,\"index\",'integrated_flux', \"event_starttime\", \"event_peaktime\", \"event_endtime\", 'noaa_ar', 'ID', 'Number','AR issue_date'], axis = 1)\n",
    "raw_data.date = pd.to_datetime(raw_data.date, format = \"%Y%m%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d95e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "goes_hierachy_index = {np.unique(raw_data.goes_class )[i]:i for i in range(len(np.unique(raw_data.goes_class )))}\n",
    "goes_hierachy = []\n",
    "for i in range(len(raw_data.goes_class)): \n",
    "    goes_hierachy.append(goes_hierachy_index[raw_data.goes_class[i]])\n",
    "raw_data.goes_class = pd.Series(goes_hierachy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a519b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARP_data = raw_data.dropna(axis = 0, subset=['MEANJZH']).reset_index()\n",
    "np.sum(SHARP_data.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d7fede",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARP_data = raw_data.dropna(axis = 0, subset=['MEANJZH']).reset_index()\n",
    "values = {\"MEANSHR\": np.mean(raw_data[\"MEANSHR\"]), \"MEANGAM\": np.mean(raw_data[\"MEANGAM\"]), \"ERRGAM\":  np.mean(raw_data[\"ERRGAM\"])}\n",
    "SHARP_data.goes_class_ind[SHARP_data.goes_class > 224] = 1\n",
    "SHARP_data.goes_class_ind[SHARP_data.goes_class <= 224] = 0\n",
    "SHARP_data = SHARP_data.fillna(values)\n",
    "SHARP_data = SHARP_data.dropna(axis = 1)\n",
    "SHARP_data = SHARP_data.replace([np.inf, -np.inf], np.nan)\n",
    "SHARP_data = SHARP_data.drop([\"index\", \"goes_class\"], axis = 1)\n",
    "SHARP_data = SHARP_data.dropna(axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7eff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARP_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a367acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exporting data \n",
    "X = np.array(SHARP_data.drop([\"date\",\"goes_class_ind\"], axis = 1))\n",
    "feature_index = SHARP_data.drop([\"date\",\"goes_class_ind\"], axis = 1).columns\n",
    "y_export = np.array(SHARP_data.goes_class_ind)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_export = scaler.fit_transform(X)\n",
    "\n",
    "# Exporting data out \n",
    "#name =  \"SHARP_NO\"\n",
    "name = \"SHARP_48\"\n",
    "for title, data in zip([\"X.pickle\", \"y.pickle\", \"index.pickle\"],[X_export , y_export, feature_index]):\n",
    "    pickle_out = open(\"clean_data/\" + name + \"_\" + title, \"wb\")\n",
    "    pickle.dump(data, pickle_out)\n",
    "    pickle_out.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f0a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(len(feature_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d6a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARP_data.drop([\"date\",\"goes_class_ind\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4878cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.isinf(np.array(raw_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd510c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(SHARP_data.goes_class_ind, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c8ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(SHARP_data.goes_class_ind, return_counts = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
